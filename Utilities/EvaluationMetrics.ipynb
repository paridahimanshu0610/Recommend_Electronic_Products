{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abff570f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    r2_score, mean_squared_error, mean_absolute_error,\n",
    "    classification_report, roc_curve, log_loss, auc, \n",
    "    precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    accuracy_score, confusion_matrix, precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7828e",
   "metadata": {},
   "source": [
    "## For Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c63b096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_model_summary(y_true, y_pred):\n",
    "    r_squared = round(r2_score(y_true, y_pred), 3)\n",
    "    rmse = round(np.sqrt(mean_squared_error(y_true, y_pred)), 3)\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 3)\n",
    "    print(\"The r-squared of the model is {} and its mean absolute error is {}.\".format(r_squared, mae))\n",
    "    \n",
    "    res = {'R2':r_squared, 'RMSE':rmse, 'MAE':mae}\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883aac97",
   "metadata": {},
   "source": [
    "## For Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13d25470",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_eval_metrics(y_true, y_pred, y_score):\n",
    "    (pr, rc, f1, _) = precision_recall_fscore_support(y_true, y_pred, average='binary', pos_label=1)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    logloss = log_loss(y_true, y_pred)\n",
    "        \n",
    "    res = {'Accuracy': acc, 'F1-Score':f1, 'Recall':rc, \n",
    "           'Precision':pr, 'ROC-AUC':roc_auc, 'Log-Loss':logloss}\n",
    "    formatted_res = {key: round(value, 3) for key, value in res.items()}\n",
    "    \n",
    "    # Creating a new dictionary with formatted values\n",
    "    print(classification_report(y_true, y_pred, target_names=['No', 'Yes']))\n",
    "    \n",
    "    return formatted_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afbb3839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_model_summary(y_true, y_pred):\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='binary', pos_label=1)\n",
    "    \n",
    "    print(f\"The accuracy of the model is {acc:.3f} and its F1 score is {f1:.3f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d8404f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(y_true, y_score, classifier_name):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_score, pos_label = 1)\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(fpr, tpr, linewidth=2)\n",
    "    plt.plot([0,1], [0,1], 'k--' )\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    plt.title('ROC curve for ' + classifier_name)\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "878c3f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_all_roc(roc_data):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for x, y in roc_data.items():  \n",
    "        classifier_name = x\n",
    "        (fpr, tpr, auc_val) = y\n",
    "        plt.plot(fpr, tpr, label=f'{classifier_name} (AUC = {auc_val:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    #plt.xlim([0.0, 1.0])\n",
    "    #plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "    plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend(loc='lower right', fontsize=8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6245a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def roc_auc_vals(y_true, y_score):\n",
    "    fpr, tpr, _ =roc_curve(y_true, y_score, pos_label = 1)\n",
    "    auc_val=auc(fpr, tpr)  \n",
    "    \n",
    "    return (fpr, tpr, auc_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f5106a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cm(y_true, y_pred):\n",
    "    cm=confusion_matrix(y_true, y_pred)\n",
    "    cm_matrix = pd.DataFrame(data=cm, columns=['Predicted Negative:0', 'Predicted Positive:1'], \n",
    "                                 index=['Actual Negative:0', 'Actual Positive:1']) \n",
    "    \n",
    "    sns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
